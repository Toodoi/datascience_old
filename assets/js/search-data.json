{
  
    
        "post0": {
            "title": "Kaggle Cassava Leaf Competition",
            "content": "This notebook briefly documents my final model for the Cassava Leaf Disease Classification competition currently on Kaggle. After building half a dozen models over the last two weeks I have achieved a maximum accuracy of 89.4% on the withheld test set. Although this accuracy is very similar to the current first place score of 90.5%, it is still a world a way in Kaggle competition terms. With this final model I hope to receive a small boost in accuracy by consolidating additional data I have found from a previous competition and will include a batch of unlabelled data. . Download and prepare the data . The competition dataset contains 21,397 images. To expand my dataset and attempt to make the model more accurate, I also downloaded an additional 5,600 labelled images from an old competition dataset. In addition to this, I found an extra 12,596 images of cassava leaves belonging to the same five classes as the competition dataset. To expand even further, I will first train a model on the labelled data and then use it for inference on the unlabelled set. Then I will take the images where the model predicted their class value with very high confidence and add them as additional pseudo-labelled data to the overall dataset. . url = &#39;https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/13836/1718836/compressed/train_images.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&amp;Expires=1607653419&amp;Signature=qVDAZiv7WLmZsHSua7PXj57fzvLpLWcDCKAHP%2BuVWvGfKUIiFoo1JZ6XZbK3Ro%2BKeSuisVoJlt5nkQAnyMX0vglWyTRSSaC%2BKwkKvjHBPVNeGtlNXLXGdixKMWePMUEgQXC1XCw8m70T3RdTxPr5NX0vi93DrhjDcciETULct%2B84akCwV551Eowsy5%2Fy%2FLXA%2FsqrIjSHnICA5O6C8eiQSDOEVsCUpNXN262mDIZ3CM3wAcx%2Fm00hyLEaNAmAlSRyr9Im7At6ORZbH2t71aET5oYWgoHcyEL280h1mOgrSVO%2F5L6cJaS4pE9m6DixJUa1J3NUBPjsfcaWjHGbJ%2BuNkg%3D%3D&amp;response-content-disposition=attachment%3B+filename%3Dtrain_images.zip&#39; wget.download(url, &#39;comp_train.zip&#39;) . &#39;comp_train.zip&#39; . !mkdir leaf !unzip comp_train.zip -d leaf/train . !unzip train2.zip -d leaf/train2 . path = Path(&#39;leaf&#39;) path.ls() . (#2) [Path(&#39;leaf/train&#39;),Path(&#39;leaf/train2&#39;)] . !unzip extraimages.zip -d leaf/extras . unlabelled_imgs = (path/&#39;extras/extraimages&#39;) unlabelled_imgs.ls() . (#12596) [Path(&#39;leaf/extras/extraimages/extra-image-13099.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-3716.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-14330.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-15689.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-14863.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-6811.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-11460.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-7662.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-14235.jpg&#39;),Path(&#39;leaf/extras/extraimages/extra-image-2074.jpg&#39;)...] . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . with open(&#39;/content/drive/My Drive/train.csv&#39;) as f: df = pd.read_csv(f) . df[&#39;image_id&#39;] = df[&#39;image_id&#39;].apply(lambda x: f&#39;train/{x}&#39;) df.head() . image_id label . 0 train/1000015157.jpg | 0 | . 1 train/1000201771.jpg | 3 | . 2 train/100042118.jpg | 1 | . 3 train/1000723321.jpg | 1 | . 4 train/1000812911.jpg | 3 | . map_classes = { &quot;0&quot;: &quot;cbb&quot;, &quot;1&quot;: &quot;cbsd&quot;, &quot;2&quot;: &quot;cgm&quot;, &quot;3&quot;: &quot;cmd&quot;, &quot;4&quot;: &quot;healthy&quot; } . df[&#39;class&#39;] = df[&#39;label&#39;].astype(str).map(map_classes) df = df.drop(columns=[&#39;label&#39;]) df.head(), df.shape . ( image_id class 0 train/1000015157.jpg cbb 1 train/1000201771.jpg cmd 2 train/100042118.jpg cbsd 3 train/1000723321.jpg cbsd 4 train/1000812911.jpg cmd, (21397, 2)) . base_train_pth = (path/&#39;train2/train&#39;) . dict_lst = [] for dir in base_train_pth.ls(): for file in dir.ls(): dir_name = file.parent.name f_path = str(file)[18:] clean_path = f_path[len(dir_name):] dict_lst.append({&#39;image_id&#39;: f&#39;train{clean_path}&#39;, &#39;class&#39;: f&#39;{dir_name}&#39;}) . Merging the two sets of labelled images, I now have 27,053 images to use for training. However I will use 15% of these images as a validation set to train the model. . df = df.append(dict_lst, True) df.shape . (27053, 2) . We can see from the graph below that the dataset is quite significantly imbalanced with cmd (Cassava Mosaic Disease) accounting for almost 16,000 images out of 27,053. However hopefully we have enough images of each type of disease for the CNN to learn enough features to make accurate predictions for each class. . sns.countplot(df[&#39;class&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe9fe42a278&gt; . I experimented a lot with different data augmentations on a subset of the data so I could quickly find which parameters were yielding better results. In the end I left most of the fastai aug_transforms function on default settings and found that blowing the image size up to 1200 and then random resize cropping down to 700 (which is around the average image size in the dataset) works well for good results. Flipping the images vertically reduced accuracy. . blocks = (ImageBlock, CategoryBlock) def get_x(r): return path/r[&#39;image_id&#39;] def get_y(r): return r[&#39;class&#39;] item_tfms = Resize(1200) batch_tfms = aug_transforms(size=700, max_lighting=0.2, flip_vert=False, max_rotate=15.0, min_scale=0.70) . main_block = DataBlock(blocks = blocks, splitter = RandomSplitter(valid_pct=0.15), get_x = get_x, get_y = get_y, item_tfms = item_tfms, batch_tfms = batch_tfms) . main_dls = main_block.dataloaders(df, bs=28) . main_dls.show_batch(max_n=6) . Training the first model . This model uses transfer learning with ResNet50 and uses cross entropy for the loss function. Fastai&#39;s fine_tune method trains the first epoch only on the randomly initialised layers that have been added to the pre-trained model and then &#39;unfreezes&#39; the other layers and continues training all layers for the remaining epochs. Here I trained for 8 epochs in total and recieved an accuracy of 89.69% on the validation set. . In my experience from training several models for this competition, the model generalises very well to the test set and the accuracy recieved when submitting to Kaggle is almost always the same as recieved on the validation set . main_learner = cnn_learner(main_dls, resnet50, metrics=accuracy).to_fp16() . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . main_learner.fine_tune(7) . epoch train_loss valid_loss accuracy time . 0 | 0.690630 | 0.588408 | 0.802317 | 19:00 | . epoch train_loss valid_loss accuracy time . 0 | 0.465602 | 0.415724 | 0.870348 | 22:28 | . 1 | 0.444149 | 0.432029 | 0.859502 | 22:13 | . 2 | 0.428360 | 0.421088 | 0.863446 | 22:25 | . 3 | 0.356730 | 0.366144 | 0.876510 | 22:16 | . 4 | 0.345323 | 0.356473 | 0.882672 | 22:39 | . 5 | 0.254021 | 0.333151 | 0.893517 | 22:43 | . 6 | 0.256352 | 0.331851 | 0.896968 | 22:26 | . Inference on the unlabelled data . Now I use the model I just trained to do inference on the extra unlabelled data. I spent some time looking through the images from the various data sources and they all seemed to be very similar. Given this, I assume my model will correctly label around 90% of the images from this new dataset. Because I plan to add these images to my current dataset for training a model, I need to minimise the amount of incorrectly labelled images as they will have a detrimental effect on the model&#39;s accuracy. . test_dls = main_dls.test_dl(unlabelled_imgs.ls()) . test_dls.show_batch(max_n=6) . preds, _ = main_learner.tta(dl=test_dls) . . df2 = pd.DataFrame(columns = [&#39;image_id&#39;, &#39;label&#39;, &#39;confidence&#39;]) . pred_vals = torch.max(preds, dim=-1) df2[&#39;label&#39;] = pred_vals.indices.numpy() df2[&#39;confidence&#39;] = pred_vals.values.numpy() . df2.head() . image_id label confidence . 0 NaN | 3 | 0.999529 | . 1 NaN | 2 | 0.854078 | . 2 NaN | 3 | 0.998108 | . 3 NaN | 3 | 0.968962 | . 4 NaN | 0 | 0.515554 | . pth_lst = [] for img_pth in unlabelled_imgs.ls(): pth_lst.append(str(img_pth)[24:]) . df2[&#39;image_id&#39;] = pth_lst . Now I have a new DataFrame with all the image names, labels and confidence scores for the unlabelled dataset. . df2.head(), df2.shape . ( image_id label confidence 0 extra-image-13099.jpg 3 0.999529 1 extra-image-3716.jpg 2 0.854078 2 extra-image-14330.jpg 3 0.998108 3 extra-image-15689.jpg 3 0.968962 4 extra-image-14863.jpg 0 0.515554, (12595, 3)) . cp -a ./leaf/extras/extraimages/. ./leaf/train/ . Most images were classified with a confidence of 90% or greater. I will try to be conservative and only take images that were classified with a confidence of 95% or greater. This means I won&#39;t be gaining as much extra data, but I can be more certain the data I am getting is accurate. . df2[&#39;confidence&#39;].plot.box() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fea101359e8&gt; . df3 = df2[df2[&#39;confidence&#39;]&gt;0.95].copy() df3.shape . (7513, 3) . df3[&#39;class&#39;] = df3[&#39;label&#39;].astype(str).map(map_classes) df3 = df3.drop(columns=[&#39;label&#39;, &#39;confidence&#39;]) df3.head(), df3.shape . df3[&#39;image_id&#39;] = df3[&#39;image_id&#39;].apply(lambda x: f&#39;extras/extraimages/{x}&#39;) df3.head() . final_data = df.append(df3) final_data . image_id class . 0 train/1000015157.jpg | cbb | . 1 train/1000201771.jpg | cmd | . 2 train/100042118.jpg | cbsd | . 3 train/1000723321.jpg | cbsd | . 4 train/1000812911.jpg | cmd | . ... ... | ... | . 12584 extras/extraimages/extra-image-11410.jpg | cmd | . 12586 extras/extraimages/extra-image-14282.jpg | cmd | . 12589 extras/extraimages/extra-image-11549.jpg | cgm | . 12592 extras/extraimages/extra-image-10956.jpg | cmd | . 12593 extras/extraimages/extra-image-2830.jpg | cmd | . 34566 rows × 2 columns . We can see that the new data is even more imbalanced with more than 6,000 of the 7,513 images being from the cmd (Cassava Mosaic Disease) class. This is likely due to the dataset being imbalanced to begin with like the other datasets but also from the model being the most confident in classifying the cmd class itself which leads to mostly cmd images getting through the 95% threshold test. Unfortunately this probably means that the new data will not be too much help, but we will continue anyway. . sns.countplot(df3[&#39;class&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe9fe3864a8&gt; . Retraining the Model . I decided to retrain the model from scratch with the full dataset. This time I used fastai&#39;s lr_find method to find a good learning rate by plotting it against the loss function on a mini-epoch. This allows me to use a greater learning rate that will train the model faster and is supposed to have better success in generalising. . final_dls = main_block.dataloaders(final_data, bs=28) . final_learner = cnn_learner(final_dls, resnet50, metrics=accuracy).to_fp16() . final_learner.loss_func = LabelSmoothingCrossEntropy() . final_learner.lr_find() . /usr/local/lib/python3.6/dist-packages/fastai/learner.py:53: UserWarning: Could not load the optimizer state. if with_opt: warn(&#34;Could not load the optimizer state.&#34;) . SuggestedLRs(lr_min=0.004786301031708717, lr_steep=1.5848931980144698e-06) . final_learner.fine_tune(2, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.909822 | 0.777522 | 0.831211 | 28:26 | . epoch train_loss valid_loss accuracy time . 0 | 0.684484 | 0.628349 | 0.896991 | 33:23 | . 1 | 0.609434 | 0.581203 | 0.915895 | 33:22 | . The high learning rate has worked well and the accuracy is 91.58% on the validation set after just three epochs. Now I unfreeze all the layers of the model, calculate a new learning rate and train further. The learning rate function is sloping upward now as the model has already learned all the &#39;low hanging fruit&#39; in its first few epochs. . final_learner.unfreeze() . final_learner.lr_find() . SuggestedLRs(lr_min=9.12010818865383e-08, lr_steep=7.585775847473997e-07) . The additional three epochs of training didn&#39;t seem to do much and the accuracy actually dropped slightly. Although the validation loss went down slightly as well so at least the model probably didn&#39;t do any overfitting on those epochs. . I saved and submitted the model to Kaggle and received a score of 88.5% for this model which marks about a 1% drop in accuracy from the models I built solely on the competition data. This might be because the additional images I found from other sources have some slight differences in them from the competition set which might lower the accuracy slightly on the competition test set. Another reason could be the addition of misclassified images from unlabelled set that I ran inference on. . final_learner.fit_one_cycle(3, lr_max=slice(6e-7, 1e-5), cbs=EarlyStoppingCallback()) . epoch train_loss valid_loss accuracy time . 0 | 0.598469 | 0.590895 | 0.912423 | 33:19 | . 1 | 0.584755 | 0.582810 | 0.912230 | 33:31 | . 2 | 0.592730 | 0.580823 | 0.914931 | 33:24 | . final_learner.save(&#39;final_model&#39;) . Path(&#39;models/final_model.pth&#39;) .",
            "url": "https://toodoi.github.io/datascience/2021/02/09/_12_08_Cassava.html",
            "relUrl": "/2021/02/09/_12_08_Cassava.html",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Baseline Model",
            "content": "Job Salary Prediction . This is a quick notebook detailing a model I built for a Kaggle competition from 2013 where Adzuna asked entrants to predict job salaries based on a dataset they supplied. This is the first model I have built which relies solely on text data which provided a good experience for working on messy data that required a lot of feature engineering. . The first few rows of the DataFrame are displayed below with summary statistics. SalaryNormalized is the variable to be predicted. . import pandas as pd from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . with open(&#39;/content/drive/My Drive/Train_reesby.csv&#39;) as f: df = pd.read_csv(f) df = df.drop(columns=[&#39;SalaryRaw&#39;, &#39;LocationRaw&#39;]) . df.head() . Id Title FullDescription LocationNormalized ContractType ContractTime Company Category SalaryNormalized SourceName . 0 12612628 | Engineering Systems Analyst | Engineering Systems Analyst Dorking Surrey Salary ****K Our client is located in Dorking, Surrey and are looking for Engineering Systems Analyst our client provides specialist software development Keywords Mathematical Modelling, Risk Analysis, System Modelling, Optimisation, MISER, PIONEEER Engineering Systems Analyst Dorking Surrey Salary ****K | Dorking | NaN | permanent | Gregory Martin International | Engineering Jobs | 25000 | cv-library.co.uk | . 1 12612830 | Stress Engineer Glasgow | Stress Engineer Glasgow Salary **** to **** We re currently looking for talented engineers to join our growing Glasgow team at a variety of levels. The roles are ideally suited to high calibre engineering graduates with any level of appropriate experience, so that we can give you the opportunity to use your technical skills to provide high quality input to our aerospace projects, spanning both aerostructures and aeroengines. In return, you can expect good career opportunities and the chance for advancement and personal and professional development, support while you gain Chartership and so... | Glasgow | NaN | permanent | Gregory Martin International | Engineering Jobs | 30000 | cv-library.co.uk | . 2 12612844 | Modelling and simulation analyst | Mathematical Modeller / Simulation Analyst / Operational Analyst Basingstoke, Hampshire Up to ****K AAE pension contribution, private medical and dental The opportunity Our client is an independent consultancy firm which has an opportunity for a Data Analyst with 35 years experience. The role will require the successful candidate to demonstrate their ability to analyse a problem and arrive at a solution, with varying levels of data being available. Essential skills Thorough knowledge of Excel and proven ability to utilise this to create powerful decision support models Experience in Modell... | Hampshire | NaN | permanent | Gregory Martin International | Engineering Jobs | 30000 | cv-library.co.uk | . 3 12613049 | Engineering Systems Analyst / Mathematical Modeller | Engineering Systems Analyst / Mathematical Modeller. Our client is a highly successful and respected Consultancy providing specialist software development MISER, PIONEER, Maths, Mathematical, Optimisation, Risk Analysis, Asset Management, Water Industry, Access, Excel, VBA, SQL, Systems . Engineering Systems Analyst / Mathematical Modeller. Salary ****K****K negotiable Location Dorking, Surrey | Surrey | NaN | permanent | Gregory Martin International | Engineering Jobs | 27500 | cv-library.co.uk | . 4 12613647 | Pioneer, Miser Engineering Systems Analyst | Pioneer, Miser Engineering Systems Analyst Dorking Surrey Salary ****K Located in Surrey, our client provides specialist software development Pioneer, Miser Engineering Systems Analyst Dorking Surrey Salary ****K | Surrey | NaN | permanent | Gregory Martin International | Engineering Jobs | 25000 | cv-library.co.uk | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 244768 entries, 0 to 244767 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 Id 244768 non-null int64 1 Title 244767 non-null object 2 FullDescription 244768 non-null object 3 LocationNormalized 244768 non-null object 4 ContractType 65442 non-null object 5 ContractTime 180863 non-null object 6 Company 212338 non-null object 7 Category 244768 non-null object 8 SalaryNormalized 244768 non-null int64 9 SourceName 244767 non-null object dtypes: int64(2), object(8) memory usage: 18.7+ MB . # Fill the one missing Title field using info from FullDescription df.at[1588, &#39;Title&#39;] = &#39;Quality Improvement Manager&#39; . baseline = np.ones(len(df[&#39;SalaryNormalized&#39;])) * df[&#39;SalaryNormalized&#39;].mean() mean_absolute_error(df[&#39;SalaryNormalized&#39;], baseline) . 13399.57843160822 . Next I test what kind of accuracy can be achieved simply by feeding a model the raw data. . I use a RandomForest because they consistently perform well on most datasets. Later on I will use a Neural Network as well. . I&#39;m using some features of the fast.ai library to encode categorical variables and easily arrange the data to feed into the models. . def splitter(df, split=0.2): ## Train/test splitting function rand_idx = list(torch.randperm(len(df)).numpy()) cut = int(split * len(df)) return rand_idx[cut:],rand_idx[:cut] splits = splitter(df) procs = [Categorify] # Encodes the categorical features dep_var = &#39;SalaryNormalized&#39; # Specify dependent variable cat_vars = [&#39;Title&#39;, &#39;FullDescription&#39;, &#39;LocationNormalized&#39;, &#39;ContractType&#39;, &#39;ContractTime&#39;, &#39;Company&#39;, &#39;Category&#39;, &#39;SourceName&#39;] to = TabularPandas(df, procs, cat_vars, y_names = dep_var, splits=splits) xs, y = to.train.xs, to.train.y valid_xs, valid_y = to.valid.xs, to.valid.y . def forest(xs, y, n_estimators=40, max_samples=175_000, min_samples_leaf=5): return RandomForestRegressor(n_estimators=n_estimators, max_samples=max_samples, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) naive_model = forest(xs, y) naive_model_preds = naive_model.predict(xs) . nm_train_score = mean_absolute_error(y, naive_model_preds) nm_valid_score = mean_absolute_error(valid_y, naive_model.predict(valid_xs)) print(&#39;The Naive Model scored a MAE of {} on the training data and {} on the validation data.&#39;.format(nm_train_score, nm_valid_score)) . The Naive Model scored a MAE of 6249.058199242024 on the training data and 8584.078321490439 on the validation data. . The RandomForest performed much better than the baseline model and has less than half the error rate when evaluated on the test set. . The cardinality of the Title feature is very high (135,436 unique values, which is about 55% of the dataset). This is because one job title, such as Systems Engineer, is represented in the dataset in many additional ways that don&#39;t carry much extra meaning or can be represented using other features (e.g. &#39;Systems Engineer London&#39; or &#39;Engineering, Systems&#39;). . To resolve this I will scrape the most common words from the job titles and assign them to each record. This should improve the model&#39;s accuracy by having considerably more records belonging to fewer, but much more predictively important titles such as Engineer, Teacher, Chef. . df[&#39;Title&#39;].nunique() . 135436 . # Counts the 500 most common words in the Title feature from collections import Counter job_word_dict = Counter(&quot; &quot;.join(df[&quot;Title&quot;]).split()).most_common(600) job_word_dict . # This cell cleans up the job_word_dict by removing non-alphanumeric entries and some &#39;joining&#39; words for i in job_word_dict: if not i[0].isalnum(): job_word_dict.remove(i) trash_words = [&#39;and&#39;, &#39;or&#39;, &#39;in&#39;, &#39;of&#39;, &#39;to&#39;, &#39;for&#39;, &#39;with&#39;] # I decided to remove these &#39;joining&#39; words and keep more informative words such as Engineer or Teacher for word in trash_words: for tup in job_word_dict: if tup[0].lower() == word.lower(): job_word_dict.remove(tup) . The 15 most common words in the job titles are shown below. &#39;London&#39; has made the list which means locations are fairly prevalent in the job title. However since I already have a location feature, I will remove them from this list. . You can also see that positions of seniority (e.g. Manager, Senior, Executive) are also very common. After experimenting with a few different combinations of features on different models, I found that splitting position seniority out into it&#39;s own feature improved the accuracy. This makes sense intuitively that having one feature to describe the role (Electrical Engineer, Software Developer) and another feature to adjust each roles seniority level would yield better results. . job_word_dict[:15] . [(&#39;Manager&#39;, 45109), (&#39;Engineer&#39;, 22161), (&#39;Sales&#39;, 17096), (&#39;Senior&#39;, 15925), (&#39;Developer&#39;, 11914), (&#39;Assistant&#39;, 10995), (&#39;Business&#39;, 9296), (&#39;Executive&#39;, 8954), (&#39;Analyst&#39;, 8387), (&#39;Consultant&#39;, 8153), (&#39;London&#39;, 8073), (&#39;Support&#39;, 7729), (&#39;Project&#39;, 6814), (&#39;Nurse&#39;, 6440), (&#39;Development&#39;, 6396)] . # This cell removes position seniority level words from the job_word_dict seniority_level = [&#39;Manager&#39;, &#39;Senior&#39;, &#39;Assistant&#39;, &#39;Executive&#39;, &#39;Advisor&#39;, &#39;Worker&#39;, &#39;Head&#39;, &#39;Graduate&#39;, &#39;Lead&#39;, &#39;Coordinator&#39;, &#39;Director&#39;, &#39;Leader&#39;, &#39;Supervisor&#39;, &#39;Junior&#39;, &#39;Trainee&#39;, &#39;Deputy&#39;] job_words = [] for tup in job_word_dict: job_words.append(tup[0]) for word in job_words: for position in seniority_level: if word.lower() == position.lower(): job_words.remove(word) print(&#39;Removed {}&#39;.format(word)) . location_list = list(df[&#39;LocationNormalized&#39;].value_counts().index) for word in job_words: for location in location_list: if word.lower() == location.lower(): job_words.remove(word) print(&#39;Removed {}&#39;.format(word)) . def findWholeWord(w): return re.compile(r&#39; b({0}) b&#39;.format(w), flags=re.IGNORECASE).search def map_function(x): result = [] for key in seniority_level: if findWholeWord(key)(x): match = findWholeWord(key)(x) if match.group().lower() not in result: result.append(match.group().lower()) else: continue return &#39; &#39;.join(result) df[&#39;seniority&#39;] = df.Title.apply(map_function) . Ten most common seniority positions in the data . df[&#39;seniority&#39;].value_counts().head(10) . 134846 manager 40354 senior 12374 assistant 8360 executive 8156 advisor 4088 worker 3493 head 3420 coordinator 2790 manager assistant 2619 Name: seniority, dtype: int64 . def map_function(x): result = [] for word in job_words: if findWholeWord(word)(x): match = findWholeWord(word)(x) if match.group().lower() not in result: result.append(match.group().lower()) else: continue return &#39; &#39;.join(result) df[&#39;job_words&#39;] = df.Title.apply(map_function) . 15 most common job word combinations in the data . df[&#39;job_words&#39;].value_counts().head(15) . 13022 sales 3065 engineer 2018 project 1818 business development 1808 account 1448 marketing 1251 consultant recruitment 1192 administrator 1054 developer 1027 finance 1019 teacher 942 accountant management 919 care 871 surveyor quantity 856 Name: job_words, dtype: int64 . Prepare the data and load it into the RandomForest model . procs = [Categorify] dep_var = &#39;SalaryNormalized&#39; cat_vars = [&#39;LocationNormalized&#39;, &#39;ContractType&#39;, &#39;ContractTime&#39;, &#39;Category&#39;, &#39;Company&#39;, &#39;SourceName&#39;, &#39;seniority&#39;, &#39;job_words&#39;] to = TabularPandas(df, procs, cat_vars, y_names = dep_var, splits=splits) xs, y = to.train.xs, to.train.y valid_xs, valid_y = to.valid.xs, to.valid.y rf_model = forest(xs, y); . forest_preds = rf_model.predict(xs) rf_train_score = mean_absolute_error(y, forest_preds) valid_preds = rf_model.predict(valid_xs) rf_valid_score = mean_absolute_error(valid_y, valid_preds) print(&#39;The Feature Engineered Model scored a MAE of {} on the training data and {} on the validation data.&#39;.format(rf_train_score, rf_valid_score)) . The Feature Engineered Model scored a MAE of 5736.43454654603 on the training data and 7574.226276235397 on the validation data. . The model has a MAE of 7574.2 on the validation set compared to a MAE of 8,584.1 that we received from feeding the model the raw data. So the feature engineering has produced an improvement to the models prediction accuracy. . Below I plot the average accuracy of the 40 estimators (trees) that make up the random forest. Starting with one estimator and moving to the average of all 40 we can see how combinations of estimators with different uncorrelated errors in aggregate produce a better accuracy. . The graph also shows the diminishing return of adding extra estimators and that moving past 40 will not have much further impact for accuracy. . preds = np.stack([t.predict(valid_xs) for t in rf_model.estimators_]) plt.plot([mean_squared_error(preds[:i+1].mean(0), valid_y, squared=False) for i in range(40)]); . The table below shows which features have the most impact on the model&#39;s accuracy. We can see that the 2 engineered features have the most importance. . ContractType is at the bottom and not an important feature to the model. This feature is binary and tells us whether a job is full or part time. However it has a huge amount of missing values which is probably why it does not yield much predictive power and can probably be removed without much loss to accuracy. . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) fi = rf_feat_importance(rf_model, xs) fi . cols imp . 7 job_words | 0.256270 | . 6 seniority | 0.199424 | . 3 Category | 0.156270 | . 5 SourceName | 0.133602 | . 4 Company | 0.096062 | . 0 LocationNormalized | 0.081501 | . 2 ContractTime | 0.065240 | . 1 ContractType | 0.011631 | . Run the model again without the ContractType feature since it has very low feature importance. . The accuracy of the model actually slightly improves on the validation set after removing this feature . cat_vars = [&#39;LocationNormalized&#39;, &#39;ContractTime&#39;, &#39;Category&#39;, &#39;Company&#39;, &#39;SourceName&#39;, &#39;seniority&#39;, &#39;job_words&#39;] # Removed ContractType to = TabularPandas(df, procs, cat_vars, y_names = dep_var, splits=splits) xs, y = to.train.xs, to.train.y valid_xs, valid_y = to.valid.xs, to.valid.y rf_model = forest(xs, y); forest_preds = rf_model.predict(xs) rf_train_score = mean_absolute_error(y, forest_preds) valid_preds = rf_model.predict(valid_xs) rf_valid_score = mean_absolute_error(valid_y, valid_preds) print(&#39;The Feature Engineered Model scored a MAE of {} on the training data and {} on the validation data.&#39;.format(rf_train_score, rf_valid_score)) . The Feature Engineered Model scored a MAE of 5755.869472062523 on the training data and 7582.1264663855745 on the validation data. . fi = rf_feat_importance(rf_model, xs) fi . cols imp . 6 job_words | 0.258271 | . 5 seniority | 0.197063 | . 2 Category | 0.155387 | . 4 SourceName | 0.142651 | . 3 Company | 0.098565 | . 0 LocationNormalized | 0.082658 | . 1 ContractTime | 0.065404 | . # This cell creates a job words feature in the DataFrame. This feature contains any words from the 500 most common Title words associated with a record def map_function(x): for word in job_words: if findWholeWord(word)(x): match = findWholeWord(word)(x) return match.group().lower() df[&#39;jword_primary&#39;] = df.Title.apply(map_function) . # This cell creates a job words feature in the DataFrame. This feature contains any words from the 500 most common Title words associated with a record def map_function(x): for word in job_words: if findWholeWord(word)(x[&#39;Title&#39;]): match = findWholeWord(word)(x[&#39;Title&#39;]) if match.group().lower() not in x[&#39;jword_primrary&#39;]: return match.group().lower() df[&#39;jword_secondary&#39;] = df.loc[:, [&#39;Title&#39;, &#39;jword_primrary&#39;]].apply(map_function, axis=1) . # This cell creates a job words feature in the DataFrame. This feature contains any words from the 500 most common Title words associated with a record def map_function(x): for word in job_words: if findWholeWord(word)(x[&#39;Title&#39;]): match = findWholeWord(word)(x[&#39;Title&#39;]) if match.group().lower() not in x[&#39;jword_primrary&#39;] and match.group().lower() not in x[&#39;jword_secondary&#39;]: return match.group().lower() df[&#39;jword_tertiary&#39;] = df.loc[:, [&#39;Title&#39;, &#39;jword_primrary&#39;, &#39;jword_secondary&#39;]].apply(map_function, axis=1) . Neural Network . Along with RandomForest, Neural Networks consistently achieve good results on most datasets. . Using the fast.ai library I can use the work I have already done cleaning and transforming the data and quickly train a neural net. . df[&#39;SalaryNormalized&#39;] = df[&#39;SalaryNormalized&#39;].astype(np.float32) dls = to.dataloaders(1024) y = to.train.y y.min(),y.max() . (5000, 200000) . learn = tabular_learner(dls, y_range=(4900,210000), layers=[500,250], n_out=1, loss_func=mae) learn.lr_find() . SuggestedLRs(lr_min=0.33113112449646, lr_steep=0.0008317637839354575) . The neural net scores considerably better than the RandomForest with a MAE of 6,230 compared to 7,625 from the best forest model . learn.fit_one_cycle(10, 7e-4) . epoch train_loss valid_loss time . 0 | 12463.211914 | 8360.904297 | 00:06 | . 1 | 7432.699707 | 7629.067383 | 00:06 | . 2 | 6397.468750 | 7158.133301 | 00:06 | . 3 | 5603.918457 | 6947.498047 | 00:06 | . 4 | 4933.529297 | 6681.348145 | 00:06 | . 5 | 4315.979492 | 6506.173340 | 00:06 | . 6 | 3694.033691 | 6318.576172 | 00:06 | . 7 | 3117.943604 | 6236.548340 | 00:06 | . 8 | 2581.269043 | 6193.021973 | 00:06 | . 9 | 2283.618896 | 6186.819824 | 00:06 | . Ensembling . Lastly I check to see if ensembling the two models will produce better results. By calculating predictions using both models and taking an average, we hope the errors in one model are uncorrelated with the errors in the other and we gain an improvement in the overall performance. . However the ensemble MAE ends up being 6,451.8 compared to the neural net&#39;s MAE of 6,186.8. I think the neural network is just too much better than the forest in this case. . nn_preds, targs = learn.get_preds() ensemble_preds = (to_np(nn_preds.squeeze()) + valid_preds) /2 mean_absolute_error(valid_y, ensemble_preds) . 6451.840949855289 . Conclusion . So on average the best model predicts salaries with an error of $6,186.8 which is a great improvement over the baseline model and has improved by over 2,000 compared to the first model I built. . Considering a lot of jobs will advertise salaries within a band of sometimes $10,000 or greater and that the same job is often advertised at different rates, I think this level of error is pretty good for the task. . There is potentially room to improve the model further by: . Improving the seniority and job_words features | Looking to mine some useful data from the FullDescription feature | Looking at ways to fill missing values in ContractType and ContractTime | Cleaning up the LocationNormalized feature a bit | Testing on more models and potentially ensembling more | . count_1 = 0 count_2 = 0 count_3 = 0 count_4 = 0 for i in df[&#39;job_words&#39;].value_counts().index: temp = i.split(&#39; &#39;) if len(temp) == 1: count_1 += 1 continue if len(temp) == 2: count_2 +=1 continue if len(temp) == 3: count_3 +=1 continue if len(temp) == 4: count_4 +=1 continue count_1, count_2, count_3, count_4 . (391, 9166, 16345, 11869) .",
            "url": "https://toodoi.github.io/datascience/2021/02/09/_01_07_JobSalaryPrediction.html",
            "relUrl": "/2021/02/09/_01_07_JobSalaryPrediction.html",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Stochastic Gradient Descent",
            "content": "!pip install fastai --upgrade . Stochastic Gradient Descent (SGD) is an optimisation algorithm which can be used to minimise a loss function in order to fit a model. This notebook illustrates the process of SGD by using it to train an image classifier from scratch. This example is taken from the fast.ai course. . The MNIST dataset is used which contains images of handwritten numbers from 0 to 9. The problem is to create a model that can correctly predict which digit a given image is. For this example the dataset is restricted to the digits 3 and 7. . from fastai.vision.all import * import seaborn as sns import pandas as pd . Downloading and preparing the dataset . The dataset contains &#39;train&#39; and &#39;valid&#39; folders each of which contain a folder for each of the two classes: 3 and 7. Create a list of tensors from the MNIST data. We have 6,131 3s, and 6,265 7s. . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() three_tensors = [tensor(Image.open(o)) for o in threes] seven_tensors = [tensor(Image.open(o)) for o in sevens] len(three_tensors),len(seven_tensors) . (6131, 6265) . Combine all the images of each class into a single three-dimensional tensor and convert the pixel values to floats between 0 and 1. Looking at the shape of the tensor, we have 6,131 images of 3s that are 28 by 28 pixels. . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . Concatenate the separate class tensors into a single tensor train_x to hold all the images for training and create the tensor train_y to hold the training labels or targets (1 for 3s and 0 for 7s). The training images and labels are zipped together in dset because a PyTorch Dataset is required to return a tuple when indexed. Note: the images have been changed from a 28 by 28 matrix to a 28*28 vector (784 column-long row) where each column represents a pixel value. This is important as it enables us to use matrix multiplication between the input data (pixel values) and parameters to produce an output. . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) dset = list(zip(train_x,train_y)) dset[0][0].shape, dset[0][1].shape . (torch.Size([784]), torch.Size([1])) . Create the validation dataset in the same manner . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . We now have two datasets: dset which contains all the training images and their associated labels, and valid_dset which contains all the validation images and their labels. The datasets are a list of tuple pairs where the first item is an image represented as a vector of magnitude 784 and the second item is a label, either 0 or 1. . Building the architecture . The first step is to initialize the parameters. These are the weights or coefficients which will be applied to the data to make class predictions. It is common to initialize the parameters randomly. Create weights, a 784 row vector corresponding to the pixel length of the images, each row has a value randomly initialized using a normal random number distribution which will be applied to the pixel values to predict the class value. bias is a randomly initialised scalar variable which increases flexibility by allowing the output of the linear equations to be non-zero when the input values are 0. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((28*28,1)) bias = init_params(1) weights.shape, bias.shape . (torch.Size([784, 1]), torch.Size([1])) . The graph below shows that each of the 784 weights has been given a random value from a standard normal distribution. . wdf = pd.DataFrame(weights) wdf.rename(columns={0 :&#39;weight value&#39;}, inplace=True ) g = sns.catplot(x=wdf.index, y=&#39;weight value&#39;, data=wdf, kind=&#39;bar&#39;) (g.set_axis_labels(&quot;Weights&quot;, &quot;Weight Value&quot;) .set_xticklabels([])) . &lt;seaborn.axisgrid.FacetGrid at 0x7ff25fe9f710&gt; . Define the model . Create a function that takes the image vectors as input and multiplies them by the parameters to produce an output which will be used to predict the class value. This simple function is actually the model and since the parameters are randomly intitialised its output won&#39;t actually have anything to do with what we want. However, once we compare the outputs with our targets and use some loss function to evaluate its performance, we can use SGD to minimise the loss and improve the model. . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[-9.8383], [-5.7327], [-0.0574], ..., [-2.0469], [-3.5765], [ 3.2822]], grad_fn=&lt;AddBackward0&gt;) . Next we set an arbitrary threshold to predict whether a given image is a 3 or a 7. . thresh = 0.0 corrects = (preds&gt;thresh).float() == train_y corrects.float().mean().item() . 0.3266376256942749 . The graph below shows the distribution of the prediction values for each class value. We can see that our arbitrary threshold of 0 in fact does a very poor job of predicting the class values because there is a healthy mix of both 3s and 7s on either end of 0. These two lovely shaped distributions are to be expected from intialising the weights randomly using the standard normal distribution. . df = pd.DataFrame(preds) df[&#39;threes&#39;] = df[:6131] df[&#39;sevens&#39;] = df[0][6131:] sns.histplot(df[&#39;threes&#39;], color=&quot;skyblue&quot;) sns.histplot(df[&#39;sevens&#39;], color=&quot;lightsalmon&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff25cdf2a58&gt; . Define a loss function . The loss function is a means of assessing the the current weight assignments in terms of actual performance. . This loss function takes the outputs from the linear1 model as inputs and compares them to the targets. It measures the distance of each prediction from its target label (i.e. 1 for 3s and 0 for 7s) for a given batch containing prediction values and target labels and returns an average of these distances. It is our goal to minimise this function by changing the values of the weights. A score of 0 would mean that for some combination of weights applied to a batch of inputs, the model produced outputs that corresponded exactly to the batch&#39;s target labels. . One problem with mnist_loss as currently defined is that it assumes that prediction values are always between 0 and 1 (the above graph shows they are actually ranging from around -20 to 20). The sigmoid method in the loss function transforms the prediction values into values that fall in the domain of 0 to 1 to allow our loss function to work. . . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . Creater the dl DataLoader which takes our dataset and turns it into an iterator over many batches. The batchs are tuples of tensors representing batches of images and targets. Here we specify a batch size of 256. Processing the dataset in batches like this is the difference between Gradient Descent and Stochastic Gradient Descent. Gradient Descent requires much more computational resources as the entire dataset needs to be processed for one epoch. . dl = DataLoader(dset, batch_size=256) valid_dl = DataLoader(valid_dset, batch_size=256) . Optimisation . Now that we have a loss function to measure the performance of our model, we need a method that updates the paramaters (weights and bias) to minimise the loss function and improve the model&#39;s performance, this is often referred to as stepping. The goal is to adjust the weights until we find the bottom of the loss function and an efficient method for this is to use the gradient of the loss function to tell us which direction to adjust the weights. . . calc_grad will process our batches through our model and assess performance using the loss function while tracking the gradient for each weight. . def calc_grad(x_batch, y_batch, model): preds = model(x_batch) loss = mnist_loss(preds, y_batch) loss.backward() . Now all that is left is to update the parameters based on the gradients. It is common to multiply the gradient by a small number called the learning rate to control the size of gradient steps. If it is too large, the algorithm could step beyond the minimum and jump around the function a lot. If it is too small, it will take longer to reach the minimum. Often times the learning rate is set through some trial and error. . The full process is represented in the flow chart below and shows how it is an iterative process. We initialise the weights and measure the predictions from a model using a loss function, record the gradients and step the parameters toward the bottom of the loss function. We repeat the process with the stepped parameters and continue iterating to improve the predictions until we decide to stop the process. . . The train_epoch function includes a loop to upgrade the parameters based on the gradients and a learning rate: lr. . def train_epoch(model, lr, params): for x_batch, y_batch in dl: calc_grad(x_batch, y_batch, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . We also want to check how we&#39;re doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it&#39;s greater than 0. The batch_accuracy function does this and returns an accuracy score. validate_epoch then puts all the batches together and returns an overall score for each epoch. . def batch_accuracy(x_batch, y_batch): preds = x_batch.sigmoid() correct = (preds&gt;0.5) == y_batch return correct.float().mean() def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . Now all thats left is to train the model. First we&#39;ll run three epochs and see how the accuracy improves and look at how the distribution of class values (targets) changes. The accuracy improves rapidly in just three epochs and we receive accuracy scores of 52.28%, 65.23% and 85%. . lr = 1. params = weights, bias for i in range(3): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.5228 0.6523 0.85 . The graph below shows the distributions of the predictions of the class values being pulled away from each other. . preds = linear1(train_x) df = pd.DataFrame(preds) df[&#39;threes&#39;] = df[:6131] df[&#39;sevens&#39;] = df[0][6131:] sns.histplot(df[&#39;threes&#39;], color=&quot;skyblue&quot;) sns.histplot(df[&#39;sevens&#39;], color=&quot;lightsalmon&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff26081e898&gt; . After 15 more epochs the accuracy improves to over 97% . for i in range(15): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.9242 0.9501 0.957 0.9609 0.9628 0.9648 0.9657 0.9682 0.9687 0.9691 0.9696 0.9701 0.9701 0.9701 0.9701 . We can also see the how the current combination of weights produces predictions which divide the class values into two distinct groups compared to the starting randomly initialised weights. The predictions of images of 7s which intially had a greater mean than the predictions of 3s has been pulled to the left into larger negative values. This is because large negative values are converted to 0 or very close to 0 by the sigmoid function. Since we set the target for 7s to 0, the weights have been optimised to the point where the model will produce large negative prediction values for images of 7s which corresponds to its target label of 0. Likewise the model will produce large positive prediction values for 3s which corresponds to its target label of 1 once the sigmoid function is applied. . preds = linear1(train_x) df = pd.DataFrame(preds) df[&#39;threes&#39;] = df[:6131] df[&#39;sevens&#39;] = df[0][6131:] sns.histplot(df[&#39;threes&#39;], color=&quot;skyblue&quot;) sns.histplot(df[&#39;sevens&#39;], color=&quot;lightsalmon&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff25d355668&gt; . Distribution of predictions using randomly initialised weights .",
            "url": "https://toodoi.github.io/datascience/2020/11/15/SGD.html",
            "relUrl": "/2020/11/15/SGD.html",
            "date": " • Nov 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! My name’s Todd. I used to work in economic and public policy and have always had a deep interest in data and programming. After returning back to Australia earlier than anticipated due to Coronavirus, I’ve had the opportunity to expand on my programming and statistics skills and make the career shift into data science which I am very passionate about. I am currently studying full time and looking for employment opportunities in the area of data science or any other jobs where I can use data driven techniques to solve problems. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://toodoi.github.io/datascience/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://toodoi.github.io/datascience/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}