{
  
    
        "post0": {
            "title": "Stochastic Gradient Descent",
            "content": "!pip install fastai --upgrade . Stochastic Gradient Descent (SGD) is an optimisation algorithm which can be used to minimise a loss function in order to fit a model. This notebook illustrates the process of SGD by using it to train an image classifier from scratch. This example is taken from the fast.ai course. . The MNIST dataset is used which contains images of handwritten numbers from 0 to 9. The problem is to create a model that can correctly predict which digit a given image is. For this example the dataset is restricted to the digits 3 and 7. . from fastai.vision.all import * import seaborn as sns import pandas as pd . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path . Downloading and preparing the dataset . The dataset contains &#39;train&#39; and &#39;valid&#39; folders each of which contain a folder for each of the two classes: 3 and 7. Create a list of tensors from the MNIST data. We have 6,131 3s, and 6,265 7s. . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() three_tensors = [tensor(Image.open(o)) for o in threes] seven_tensors = [tensor(Image.open(o)) for o in sevens] len(three_tensors),len(seven_tensors) . (6131, 6265) . Combine all the images of each class into a single three-dimensional tensor and convert the pixel values to floats between 0 and 1. Looking at the shape of the tensor, we have 6,131 images of 3s that are 28 by 28 pixels. . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . Concatenate the separate class tensors into a single tensor train_x to hold all the images for training and create the tensor train_y to hold the training labels or targets (1 for 3s and 0 for 7s). The training images and labels are zipped together in dset because a PyTorch Dataset is required to return a tuple when indexed. Note: the images have been changed from a 28 by 28 matrix to a 28*28 vector (784 column-long row) where each column represents a pixel value. This is important as it enables us to use matrix multiplication between the input data (pixel values) and parameters to produce an output. . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) dset = list(zip(train_x,train_y)) dset[0][0].shape, dset[0][1].shape . (torch.Size([784]), torch.Size([1])) . Create the validation dataset in the same manner . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . We now have two datasets: dset which contains all the training images and their associated labels, and valid_dset which contains all the validation images and their labels. The datasets are a list of tuple pairs where the first item is an image represented as a vector of magnitude 784 and the second item is a label, either 0 or 1. . Building the architecture . The first step is to initialize the parameters. These are the weights or coefficients which will be applied to the data to make class predictions. It is common to initialize the parameters randomly. Create weights, a 784 row vector corresponding to the pixel length of the images, each row has a value randomly initialized using a normal random number distribution which will be applied to the pixel values to predict the class value. bias is a randomly initialised scalar variable which increases flexibility by allowing the output of the linear equations to be non-zero when the input values are 0. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((28*28,1)) bias = init_params(1) weights.shape, bias.shape . (torch.Size([784, 1]), torch.Size([1])) . The graph below shows that each of the 784 weights has been given a random value from a standard normal distribution. . wdf = pd.DataFrame(weights) wdf.rename(columns={0 :&#39;weight value&#39;}, inplace=True ) g = sns.catplot(x=wdf.index, y=&#39;weight value&#39;, data=wdf, kind=&#39;bar&#39;) (g.set_axis_labels(&quot;Weights&quot;, &quot;Weight Value&quot;) .set_xticklabels([])) . &lt;seaborn.axisgrid.FacetGrid at 0x7ff25fe9f710&gt; . Define the model . Create a function that takes the image vectors as input and multiplies them by the parameters to produce an output which will be used to predict the class value. This simple function is actually the model and since the parameters are randomly intitialised its output won&#39;t actually have anything to do with what we want. However, once we compare the outputs with our targets and use some loss function to evaluate its performance, we can use SGD to minimise the loss and improve the model. . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[-9.8383], [-5.7327], [-0.0574], ..., [-2.0469], [-3.5765], [ 3.2822]], grad_fn=&lt;AddBackward0&gt;) . Next we set an arbitrary threshold to predict whether a given image is a 3 or a 7. . thresh = 0.0 corrects = (preds&gt;thresh).float() == train_y corrects.float().mean().item() . 0.3266376256942749 . The graph below shows the distribution of the prediction outputs for each class value. We can see that our arbitrary threshold of 0 in fact does a very poor job of predicting the class values because there is a healthy mix of both 3s and 7s on either end of 0. These two nicely distributions are to be expected from intialising the weights using a normal distribution. . df = pd.DataFrame(preds) df[&#39;threes&#39;] = df[:6131] df[&#39;sevens&#39;] = df[0][6131:] sns.histplot(df[&#39;threes&#39;], color=&quot;skyblue&quot;) sns.histplot(df[&#39;sevens&#39;], color=&quot;lightsalmon&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff25cdf2a58&gt; . Define a loss function . The loss function is a means of assessing the the current weight assignments in terms of actual performance. . This loss function takes the outputs from the linear1 model as inputs and compares them to the targets. It measures the distance of each prediction from its target label (i.e. 1 for 3s and 0 for 7s) for a batch of predictions and targets and returns an average of these distances. It is our goal to minimise this function by changing the values of the weights. A score of 0 would mean that for some combination of weights applied to a batch of inputs, the model produced outputs that corresponded exactly to the batch&#39;s target labels. . One problem with mnist_loss as currently defined is that it assumes that prediction values are always between 0 and 1 (the above graph shows they are actually ranging from around -20 to 20). The sigmoid method in the loss function transforms the prediction values into values that fall in the domain of 0 to 1 to allow our loss function to work. . . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . Creater the dl DataLoader which takes our dataset and turns it into an iterator over many batches. The batchs are tuples of tensors representing batches of images and targets. Here we specify a batch size of 256. Processing the dataset in batches like this is the difference between Gradient Descent and Stochastic Gradient Descent. Gradient Descent requires much more computational resources as the entire dataset needs to be processed for one epoch. . dl = DataLoader(dset, batch_size=256) valid_dl = DataLoader(valid_dset, batch_size=256) . Optimisation . Now that we have a loss function to measure the performance of our model, we need a method that updates the paramaters (weights and bias) to minimise the loss function and improve the model&#39;s performance, this is often referred to as stepping. The goal is to adjust the weights until we find the bottom of the loss function and an efficient method for this is to use the gradient of the loss function to tell us which direction to adjust the weights. . . calc_grad will process our batches through our model and assess performance using the loss function while tracking the gradient for each weight. . def calc_grad(x_batch, y_batch, model): preds = model(x_batch) loss = mnist_loss(preds, y_batch) loss.backward() . Now all that is left is to update the parameters based on the gradients. It is common to multiply the gradient by a small number called the learning rate to control the size of gradient steps. If it is too large, the algorithm could step beyond the minimum and jump around the function a lot. If it is too small, it will take longer to reach the minimum. Often times the learning rate is set through some trial and error. . The full process is represented in the flow chart below and shows how it is an iterative process. We initialise the weights and measure the predictions from a model using a loss function, record the gradients and step the parameters toward the bottom of the loss function. We repeat the process with the stepped parameters and continue iterating to improve the predictions until we decide to stop the process. . . The train_epoch function includes a loop to upgrade the parameters based on the gradients and a learning rate: lr. . def train_epoch(model, lr, params): for x_batch, y_batch in dl: calc_grad(x_batch, y_batch, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . We also want to check how we&#39;re doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it&#39;s greater than 0. The batch_accuracy function does this and returns an accuracy score. validate_epoch then puts all the batches together and returns an overall score for each epoch. . def batch_accuracy(x_batch, y_batch): preds = x_batch.sigmoid() correct = (preds&gt;0.5) == y_batch return correct.float().mean() def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . Now all thats left is to train the model. First we&#39;ll run three epochs and see how the accuracy improves and look at how the distribution of class values (targets) changes. The accuracy improves rapidly in just three epochs and we receive accuracy scores of 52.28%, 65.23% and 85%. . lr = 1. params = weights, bias for i in range(3): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.5228 0.6523 0.85 . The graph below shows the distributions of the predictions of the class values being pulled away from each other. . preds = linear1(train_x) df = pd.DataFrame(preds) df[&#39;threes&#39;] = df[:6131] df[&#39;sevens&#39;] = df[0][6131:] sns.histplot(df[&#39;threes&#39;], color=&quot;skyblue&quot;) sns.histplot(df[&#39;sevens&#39;], color=&quot;lightsalmon&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff26081e898&gt; . After 15 more epochs the accuracy improves to over 97% . for i in range(15): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.9242 0.9501 0.957 0.9609 0.9628 0.9648 0.9657 0.9682 0.9687 0.9691 0.9696 0.9701 0.9701 0.9701 0.9701 . We can also see the how the current combination of weights produces predictions which divide the class values into two distinct groups compared to the starting randomly initialised weights. The predictions of images of 7s which intially had a greater mean than the predictions of 3s has been pulled to the left into larger negative values. This is because large negative values are converted to 0 or very close to 0 by the sigmoid function. Since we set the target for 7s to 0, the weights have been optimised to the point where the model will produce large negative prediction values for images of 7s which corresponds to its target label of 0. Likewise the model will produce large positive prediction values for 3s which corresponds to its target label of 1 once the sigmoid function is applied. . preds = linear1(train_x) df = pd.DataFrame(preds) df[&#39;threes&#39;] = df[:6131] df[&#39;sevens&#39;] = df[0][6131:] sns.histplot(df[&#39;threes&#39;], color=&quot;skyblue&quot;) sns.histplot(df[&#39;sevens&#39;], color=&quot;lightsalmon&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff25d355668&gt; . Distribution of predictions using randomly initialised weights .",
            "url": "https://toodoi.github.io/datascience/2020/11/14/_11_15_SGD.html",
            "relUrl": "/2020/11/14/_11_15_SGD.html",
            "date": " • Nov 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://toodoi.github.io/datascience/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://toodoi.github.io/datascience/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://toodoi.github.io/datascience/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://toodoi.github.io/datascience/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}